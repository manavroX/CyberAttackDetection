{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerform a statistic analysis of the Neural network classifier.\\n\\nParameters\\n----------\\ndata_window_botnetx.h5         : extracted data from preprocessing1.py\\ndata_window3_botnetx.h5        : extracted data from preprocessing2.py\\ndata_window_botnetx_labels.npy : label numpy array from preprocessing1.py\\nnb_prediction                  : number of predictions to perform\\n\\nReturn\\n----------\\nPrint train and test mean accuracy, precison, recall, f1\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Antoine DELPLACE\n",
    "# Last update: 17/01/2020\n",
    "\"\"\"\n",
    "Perform a statistic analysis of the Neural network classifier.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "data_window_botnetx.h5         : extracted data from preprocessing1.py\n",
    "data_window3_botnetx.h5        : extracted data from preprocessing2.py\n",
    "data_window_botnetx_labels.npy : label numpy array from preprocessing1.py\n",
    "nb_prediction                  : number of predictions to perform\n",
    "\n",
    "Return\n",
    "----------\n",
    "Print train and test mean accuracy, precison, recall, f1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import h5py\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (2.6.0)\n",
      "Collecting keras\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.2 requires keras<2.7,>=2.6.0, but you have keras 2.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed keras-2.8.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (2.6.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: clang~=5.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (0.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: cached-property in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2018.1.18)\n",
      "Requirement already satisfied: dataclasses in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/2019a7ps1343h/.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.2)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "Successfully installed keras-2.6.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install --upgrade keras\n",
    "! python3 -m pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, Dense, Dropout, Flatten, Lambda, MaxPool2D, Conv2DTranspose, UpSampling2D, Concatenate, Add\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, feature_selection, utils, ensemble, linear_model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import data\n"
     ]
    }
   ],
   "source": [
    "print(\"Import data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf('data_window_botnet3.h5', key='data')\n",
    "X.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.read_hdf('data_window3_botnet3.h5', key='data')\n",
    "X2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.join(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('window_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['Label_<lambda>']\n",
    "X.drop('Label_<lambda>', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"data_window_botnet3_labels.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['counts' 'Sport_nunique' 'DstAddr_nunique' 'Dport_nunique' 'Dur_sum'\n",
      " 'Dur_mean' 'Dur_std' 'Dur_max' 'Dur_median' 'TotBytes_sum'\n",
      " 'TotBytes_mean' 'TotBytes_std' 'TotBytes_max' 'TotBytes_median'\n",
      " 'SrcBytes_sum' 'SrcBytes_mean' 'SrcBytes_std' 'SrcBytes_max'\n",
      " 'SrcBytes_median' 'Sport_RU' 'DstAddr_RU' 'Dport_RU']\n",
      "['flow=Background' 'flow=To-Backgro' 'flow=From-Backg' 'flow=From-Norma'\n",
      " 'flow=To-Normal-' 'flow=Normal-V42' 'flow=From-Botne']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(X.columns.values)\n",
    "print(labels)\n",
    "print(np.where(labels == 'flow=From-Botne')[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (array([list([0]), list([1]), list([2]), list([3]), list([4]), list([6])],\n",
      "      dtype=object), array([2207092,   18047,     263,     984,      48,     286]))\n"
     ]
    }
   ],
   "source": [
    "y_bin6 = y==np.where(labels == 'flow=From-Botne')[0][0]\n",
    "print(\"y\", np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## NN\n",
    "filename_weights = \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fprecision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def frecall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ff1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = fprecision(y_true, y_pred)\n",
    "    r = frecall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_model(inputs, dropout=0.5, batchnorm=True):\n",
    "    x = Dense(256, input_shape=(22,))(inputs)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Dense(128, input_shape=(256,))(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Dense(1, input_shape=(128,))(x)\n",
    "    outputs = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[545331265  64051946 930796018]\n"
     ]
    }
   ],
   "source": [
    "nb_prediction = 3\n",
    "np.random.seed(seed=123456)\n",
    "tab_seed = np.random.randint(0, 1000000000, nb_prediction)\n",
    "print(tab_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_train_precision = np.array([0.]*nb_prediction)\n",
    "tab_train_recall = np.array([0.]*nb_prediction)\n",
    "tab_train_fbeta_score = np.array([0.]*nb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test_precision = np.array([0.]*nb_prediction)\n",
    "tab_test_recall = np.array([0.]*nb_prediction)\n",
    "tab_test_fbeta_score = np.array([0.]*nb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_train (array([False,  True]), array([1491711,     191]))\n",
      "y_test (array([False,  True]), array([734723,     95]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2019a7ps1343h/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00073, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00073\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00073 to 0.00069, saving model to model.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00069 to 0.00062, saving model to model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00062 to 0.00055, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00055 to 0.00050, saving model to model.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00050 to 0.00043, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00043\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00043 to 0.00014, saving model to model.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00014\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00014\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00014\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00014\n",
      "Execution time =  2504.5405509471893\n",
      "1\n",
      "y_train (array([False,  True]), array([1491718,     184]))\n",
      "y_test (array([False,  True]), array([734716,    102]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2019a7ps1343h/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00055, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00055\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00055 to 0.00042, saving model to model.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00042 to 0.00035, saving model to model.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00035 to 0.00019, saving model to model.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00019\n",
      "Execution time =  2619.6403210163116\n",
      "2\n",
      "y_train (array([False,  True]), array([1491713,     189]))\n",
      "y_test (array([False,  True]), array([734721,     97]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2019a7ps1343h/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00234, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00234\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00234 to 0.00210, saving model to model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00210 to 0.00191, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00191 to 0.00059, saving model to model.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00059\n",
      "Execution time =  2538.656158685684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/2019a7ps1343h/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/2019a7ps1343h/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, nb_prediction):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_bin6, test_size=0.33, random_state=tab_seed[i])\n",
    "\n",
    "    print(i)\n",
    "    print(\"y_train\", np.unique(y_train, return_counts=True))\n",
    "    print(\"y_test\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "    inputs = Input((22,), name='input')\n",
    "    model = get_model(inputs, dropout=0, batchnorm=1)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filename_weights, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3), loss=[\"binary_crossentropy\"], metrics=[fprecision, frecall, ff1_score])\n",
    "    #model.summary()\n",
    "\n",
    "    tps = time.time()\n",
    "    results = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_split=0.15, shuffle=True, class_weight=None, verbose=0, callbacks=callbacks)\n",
    "    print(\"Execution time = \", time.time()-tps)\n",
    "\n",
    "    model.load_weights(filename_weights)\n",
    "\n",
    "    y_pred_train = model.predict(X_train, batch_size=32, verbose=0)\n",
    "    y_pred_train_bin = (y_pred_train > 0.5).astype(np.uint8)\n",
    "    precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(y_train, y_pred_train_bin)\n",
    "    tab_train_precision[i] = precision[1]\n",
    "    tab_train_recall[i] = recall[1]\n",
    "    tab_train_fbeta_score[i] = fbeta_score[1]\n",
    "\n",
    "    y_pred_test = model.predict(X_test, batch_size=32, verbose=0)\n",
    "    y_pred_test_bin = (y_pred_test > 0.5).astype(np.uint8)\n",
    "    precision, recall, fbeta_score, support = metrics.precision_recall_fscore_support(y_test, y_pred_test_bin)\n",
    "    tab_test_precision[i] = precision[1]\n",
    "    tab_test_recall[i] = recall[1]\n",
    "    tab_test_fbeta_score[i] = fbeta_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "precision =  0.653558052434457 0.46241411916332475 [1.         0.96067416 0.        ]\n",
      "recall =  0.5802887168980955 0.41313613097393037 [0.81151832 0.92934783 0.        ]\n",
      "fbeta_score =  0.613568379480301 0.4343154915115127 [0.89595376 0.94475138 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\")\n",
    "print(\"precision = \", tab_train_precision.mean(), tab_train_precision.std(), tab_train_precision)\n",
    "print(\"recall = \", tab_train_recall.mean(), tab_train_recall.std(), tab_train_recall)\n",
    "print(\"fbeta_score = \", tab_train_fbeta_score.mean(), tab_train_fbeta_score.std(), tab_train_fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "precision =  0.6564625850340136 0.4643573491371904 [1.         0.96938776 0.        ]\n",
      "recall =  0.5841417268661852 0.41549872751163763 [0.82105263 0.93137255 0.        ]\n",
      "fbeta_score =  0.6172447013487475 0.43690247972545937 [0.9017341 0.95      0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")\n",
    "print(\"precision = \", tab_test_precision.mean(), tab_test_precision.std(), tab_test_precision)\n",
    "print(\"recall = \", tab_test_recall.mean(), tab_test_recall.std(), tab_test_recall)\n",
    "print(\"fbeta_score = \", tab_test_fbeta_score.mean(), tab_test_fbeta_score.std(), tab_test_fbeta_score)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
